### 哈希表的原理 Top K 算法详解， MPQ中的Hash算法
```
问题: 海量日志数据, 提取出某日访问百度次数最多的那个IP.

    首先是这一天, 并且是访问百度的日志中的IP取出来, 逐个写入到一个大文件中. 注意到IP是32位的, 最多有2^32个不同的IP(事实上不可能, 得扣去特殊的IP), 在重复量小的情况下, 内存放不下这些不同的IP. 同样可以采用映射的方法, 比如模1000, 把整个大文件映射为1000个小文件, 再找出每个小文件中出现频率最大的IP(可以采用hash_map进行频率统计, 然后再找出频率最大的几个)及相应的频率. 然后再在这1000个最大的IP中, 找出那个频率最大的IP, 即为所求.

具体做法如下：

    按照IP地址的Hash(IP)%1024值, 把海量IP日志分别存储到1024个小文件中.
    对于每一个小文件, 构建一个以IP为key, 出现次数为value的HashMap, 同时记录当前出现次数最多的那个IP地址;
    得到1024个小文件中的出现次数最多的IP, 再依据常规的排序算法得到总体上出现次数最多的IP.
21Hash算法以及暴雪Hash
2015年06月12日 09:08:03 阅读数：1803更多
个人分类： 算法 July文章

一：哈希表简介

    哈希表是一种查找效率极高的数据结构，理想情况下哈希表插入和查找操作的时间复杂度均为O(1)，任何一个数据项可以在一个与哈希表长度无关的时间内计算出一个哈希值（key），然后在常量时间内定位到一个桶（术语bucket，表示哈希表中的一个位置）。当然这是理想情况下，因为任何哈希表的长度都是有限的，所以一定存在不同的数据项具有相同哈希值的情况，此时不同数据项被定为到同一个桶，称为碰撞（collision）。

    哈希表的实现需要解决碰撞问题，碰撞解决大体有两种思路：

    第一种是根据某种原则将被碰撞数据定为到其它桶，例如线性探测——如果数据在插入时发生了碰撞，则顺序查找这个桶后面的桶，将其放入第一个没有被使用的桶；

    第二种策略是每个桶不是一个只能容纳单个数据项的位置，而是一个可容纳多个数据的数据结构（例如链表或红黑树），所有碰撞的数据以某种数据结构的形式组织起来。

    不论使用了哪种碰撞解决策略，都导致插入和查找操作的时间复杂度不再是O(1)。以查找为例，不能通过key定位到桶就结束，必须还要比较原始数据项是否相等，如果不相等，则要使用与插入相同的算法继续查找，直到找到匹配的值或确认数据不在哈希表中。

    使用单链表解决碰撞的哈希表，平均查找复杂度为O(L)，其中L为桶链表的平均长度；而最坏复杂度为O(N)，此时所有数据全部碰撞，哈希表退化成单链表。如下图：

 

二：暴雪的Hash算法(MPQ)

    由一个简单的问题逐步入手：有一个庞大的字符串数组，数组元素就是字符串，然后给定一个单独的字符串，让你从这个数组中查找是否有这个字符串并找到它，你会怎么做？

    有一个方法最简单，老老实实从头查到尾，一个一个比较，直到找到为止，这样做的效率极低。

    最合适的算法自然是使用HashTable（哈希表），可以把一个字符串"压缩" 成一个整数。在暴雪的HASH算法中，两个字符串计算出的Hash值相等的可能非常小，下面看看在MPQ中的Hash算法：

1：函数prepareCryptTable生成一个长度为0x500（合10进制数：1280）的cryptTable[0x500]

 

void prepareCryptTable()  

{   

         unsigned long seed = 0x00100001, index1 = 0, index2 = 0, i;  

  

         for( index1 = 0; index1 < 0x100; index1++ )  

         {   

                  for( index2 = index1, i = 0; i < 5; i++, index2 += 0x100 )  

                  {   

                          unsigned long temp1, temp2;  

  

                          seed = (seed * 125 + 3) % 0x2AAAAB;  

                          temp1 = (seed & 0xFFFF) << 0x10;  

  

                          seed = (seed * 125 + 3) % 0x2AAAAB;  

                          temp2 = (seed & 0xFFFF);  

  

                          cryptTable[index2] = ( temp1 | temp2 );   

                  }   

         }   

}   

 

2：函数HashString计算字符串lpszFileName的hash值，其中dwHashType 为hash的类型。

unsigned long HashString(const char *lpszkeyName, unsigned long dwHashType )  

{  

         unsigned char *key  = (unsigned char *)lpszkeyName;  

         unsigned long seed1 = 0x7FED7FED;  

         unsigned long seed2 = 0xEEEEEEEE;  

         int ch;  

  

         while( *key != 0 )  

         {  

                  ch = *key++;  

                  seed1 = cryptTable[(dwHashType<<8) + ch] ^ (seed1 + seed2);  

                  seed2 = ch + seed1 + seed2 + (seed2<<5) + 3;  

         }  

         return seed1;  

}  

    Blizzard的这个算法是非常高效的，被称为"One-WayHash"( 即通过HASH值反推字符串几乎是不可能的)。举个例子，字符串"unitneutralacritter.grp"通过这个算法得到的结果是0xA26067F3。

    然后是构造一个哈希表来解决问题，哈希表是一个大数组，这个数组的容量根据程序的要求来定义，例如1024。

    每一个Hash值通过取模运算 (mod) 对应到数组中的一个位置，这样，只要比较这个字符串的哈希值对应的位置有没有被占用，就可以得到最后的结果了，

#if 0

想想这是什么速度？是的，是最快的O(1)，现在仔细看看这个算法吧：

typedef struct  

{  

         int nHashA;  

         int nHashB;  

         char bExists;  

         ......  

} SOMESTRUCTRUE;  

  

3：函数GetHashTablePos在Hash表中查找是否存在目标字符串，有则返回要查找字符串的Hash值，否则，return -1.

int GetHashTablePos( char *lpszString, SOMESTRUCTURE *lpTable )   

{   

         //调用上述函数HashString，返回要查找字符串lpszString的Hash值。  

         int nHash = HashString(lpszString);      int nHashPos = nHash % nTableSize;  

   

         if ( lpTable[nHashPos].bExists  &&  !strcmp( lpTable[nHashPos].pString, lpszString ) )   

         {         

                  return nHashPos;    //返回找到的Hash值  

         }   

         else  

         {  

                  return -1;    

         }   

}  

#endif

    看到此，我想大家都在想一个很严重的问题：如果两个字符串在哈希表中对应的位置相同怎么办？毕竟一个数组容量是有限的，这种可能性很大。

    Blizzard的程序员使用精妙的方法。基本原理就是：他们在哈希表中不是用一个哈希值而是用三个哈希值来校验字符串。

 

    MPQ 使用的哈希表的格式与正常的哈希表有一些不同。它没有把实际的文件名存储在表中用于验证，实际上它根本就没有存储文件名。而是使用了3种不同的哈希：一个用于哈希表的下标，两个用于验证。这两个验证哈希替代了实际文件名。

    假如说两个不同的字符串经过一个哈希算法得到的入口点一致有可能，但用三个不同的哈希算法算出的入口点都一致，那几乎可以肯定是不可能的事了。当然了，这样仍然会出现2个不同的文件名哈希到3个同样的哈希。但是这种情况发生的概率平均是：1:18889465931478580854784，这个概率对于任何人来说应该都是足够小的。现在再回到数据结构上，Blizzard使用的哈希表没有使用链表，而采用"顺延"的方式来解决问题。

4：函数GetHashTablePos中，lpszString 为要在hash表中查找的字符串；lpTable 为存储字符串hash值的hash表；nTableSize为hash表的长度： 

 

int GetHashTablePos( char *lpszString, MPQHASHTABLE *lpTable, int nTableSize )  

{  

         const int  HASH_OFFSET = 0, HASH_A = 1, HASH_B = 2;  

   

         int  nHash = HashString( lpszString, HASH_OFFSET );  

         int  nHashA = HashString( lpszString, HASH_A );  

         int  nHashB = HashString( lpszString, HASH_B );  

         int  nHashStart = nHash % nTableSize;  

         int  nHashPos = nHashStart;  

   

         while ( lpTable[nHashPos].bExists )  

         {  

                  if (lpTable[nHashPos].nHashA == nHashA && lpTable[nHashPos].nHashB == nHashB )  

                  {  

                           return nHashPos;  

                  }  

                  else  

                  {  

                          nHashPos = (nHashPos + 1) % nTableSize;  

                  }  

   

                  if (nHashPos == nHashStart)  

                          break;  

         }  

         return -1;  

}  

   

上述程序解释：

    1计算出字符串的三个哈希值（一个用来确定位置，另外两个用来校验)

    2察看哈希表中的这个位置

    3哈希表中这个位置为空吗？如果为空，则肯定该字符串不存在，返回-1。

    4如果存在，则检查其他两个哈希值是否也匹配，如果匹配，则表示找到了该字符串，返回其Hash值。

    5移到下一个位置，如果已经移到了表的末尾，则反绕到表的开始位置起继续查询　

    6看看是不是又回到了原来的位置，如果是，则返回没找到

    7回到3。

 

三：哈希表大小

    哈希表的数组是定长的，如果太大，则浪费，如果太小，体现不出效率。合适的数组大小是哈希表的性能的关键。

    哈希表的尺寸最好是一个质数。当然，根据不同的数据量，会有不同的哈希表的大小。对于数据量时多时少的应用，最好的设计是使用动态可变尺寸的哈希表，那么如果你发现哈希表尺寸太小了，比如其中的元素是哈希表尺寸的2倍时，我们就需要扩大哈希表尺寸，一般是扩大一倍。

    下面是哈希表尺寸大小的可能取值：

    17, 37, 79, 163, 331, 673, 1361, 2729, 5471,10949, 21911, 43853, 87719, 175447, 350899, 701819, 1403641, 2807303, 5614657, 11229331,22458671, 44917381, 89834777, 179669557, 359339171, 718678369, 1437356741, 2147483647

```

### 死锁是什么？如何避免死锁
```
避免死锁的几种方式：

    设置加锁顺序

    设置加锁时限

    死锁检测

设置加锁顺序（线程按照一定的顺序加锁）：

死锁发生在多个线程需要相同的锁,但是获得不同的顺序。

假如一个线程需要锁，那么他必须按照一定得顺序获得锁。
例如加锁顺序是A->B->C，现在想要线程C想要获取锁，那么他必须等到线程A和线程B获取锁之后才能轮到他获取。（排队执行，获取锁）

缺点：
按照顺序加锁是一种有效的死锁预防机制。但是，这种方式需要你事先知道所有可能会用到的锁，并知道他们之间获取锁的顺序是什么样的。
设置加锁时限：（超时重试）

在获取锁的时候尝试加一个获取锁的时限，超过时限不需要再获取锁，放弃操作（对锁的请求。）。

若一个线程在一定的时间里没有成功的获取到锁，则会进行回退并释放之前获取到的锁，然后等待一段时间后进行重试。在这段等待时间中其他线程有机会尝试获取相同的锁，这样就能保证在没有获取锁的时候继续执行比的事情。

缺点：
但是由于存在锁的超时，通过设置时限并不能确定出现了死锁，每种方法总是有缺陷的。有时为了执行某个任务。某个线程花了很长的时间去执行任务，如果在其他线程看来，可能这个时间已经超过了等待的时限，可能出现了死锁。

在大量线程去操作相同的资源的时候，这个情况又是一个不可避免的事情，比如说，现在只有两个线程，一个线程执行的时候，超过了等待的时间，下一个线程会尝试获取相同的锁，避免出现死锁。但是这时候不是两个线程了，可能是几百个线程同时去执行，大的基数让事件出现的概率变大，假如线程还是等待那么长时间，但是多个线程的等待时间就有可能重叠，因此又会出现竞争超时，由于他们的超时发生时间正好赶在了一起，而超时等待的时间又是一致的，那么他们下一次又会竞争，等待，这就又出现了死锁。
死锁检测：

当一个线程获取锁的时候，会在相应的数据结构中记录下来，相同下，如果有线程请求锁，也会在相应的结构中记录下来。当一个线程请求失败时，需要遍历一下这个数据结构检查是否有死锁产生。

例如：线程A请求锁住一个方法1，但是现在这个方法是线程B所有的，这时候线程A可以检查一下线程B是否已经请求了线程A当前所持有的锁，像是一个环，线程A拥有锁1，请求锁2，线程B拥有锁2，请求锁1。
当遍历这个存储结构的时候，如果发现了死锁，一个可行的办法就是释放所有的锁，回退，并且等待一段时间后再次尝试。

缺点：
这个这个方法和上面的超时重试的策略是一样的。但是在大量线程的时候问题还是会出现和设置加锁时限相同的问题。每次线程之间发生竞争。
还有一种解决方法是设置线程优先级，这样其中几个线程回退，其余的线程继续保持着他们获取的锁，也可以尝试随机设置优先级，这样保证线程的执行。

参考引用：http://ifeve.com/deadlock-prevention/


教科书般的回答应该是，结合“哲学家就餐[1]”模型，分析并总结出以下死锁的原因，最后得出“避免死锁就是破坏造成死锁的，若干条件中的任意一个”的结论。

造成死锁必须达成的4个条件（原因）：

    互斥条件：一个资源每次只能被一个线程使用。
    请求与保持条件：一个线程因请求资源而阻塞时，对已获得的资源保持不放。
    不剥夺条件：线程已获得的资源，在未使用完之前，不能强行剥夺。
    循环等待条件：若干线程之间形成一种头尾相接的循环等待资源关系。

但是，“哲学家就餐”光看名字就很讨厌，然后以上这4个条件看起来也很绕口，再加上笔者又是个懒人，所以要让我在面试时把这些“背诵”出来实在是太难了！必须要想办法把这4个条件简化一下！
于是，通过对4个造成死锁的条件进行逐条分析，我们可以得出以下4个结论。

    互斥条件 ---> 独占锁的特点之一。
    请求与保持条件 ---> 独占锁的特点之一，尝试获取锁时并不会释放已经持有的锁
    不剥夺条件 ---> 独占锁的特点之一。
    循环等待条件 ---> 唯一需要记忆的造成死锁的条件。

不错！复杂的死锁条件经过简化，现在需要记忆的仅只有独占锁与第四个条件而已。

所以，面对如何避免死锁这个问题，我们只需要这样回答！
: 在并发程序中，避免了逻辑中出现复数个线程互相持有对方线程所需要的独占锁的的情况，就可以避免死锁。

作者：给你添麻烦了
链接：https://www.jianshu.com/p/44125bb12ebf
來源：简书
简书著作权归作者所有，任何形式的转载都请联系作者获得授权并注明出处。

```

### TCP和UDP的最完整的区别
```
TCP UDP
TCP与UDP基本区别
  1.基于连接与无连接
  2.TCP要求系统资源较多，UDP较少； 
  3.UDP程序结构较简单 
  4.流模式（TCP）与数据报模式(UDP); 
  5.TCP保证数据正确性，UDP可能丢包 
  6.TCP保证数据顺序，UDP不保证 
　　
UDP应用场景：
  1.面向数据报方式
  2.网络数据大多为短消息 
  3.拥有大量Client
  4.对数据安全性无特殊要求
  5.网络负担非常重，但对响应速度要求高
 
具体编程时的区别
   1.socket()的参数不同 
　　 2.UDP Server不需要调用listen和accept 
　　 3.UDP收发数据用sendto/recvfrom函数 
　　 4.TCP：地址信息在connect/accept时确定 
　　 5.UDP：在sendto/recvfrom函数中每次均 需指定地址信息 
　　 6.UDP：shutdown函数无效
 
编程区别
   通常我们在说到网络编程时默认是指TCP编程，即用前面提到的socket函数创建一个socket用于TCP通讯，函数参数我们通常填为SOCK_STREAM。即socket(PF_INET, SOCK_STREAM, 0)，这表示建立一个socket用于流式网络通讯。 
　  SOCK_STREAM这种的特点是面向连接的，即每次收发数据之前必须通过connect建立连接，也是双向的，即任何一方都可以收发数据，协议本身提供了一些保障机制保证它是可靠的、有序的，即每个包按照发送的顺序到达接收方。 

　　而SOCK_DGRAM这种是User Datagram Protocol协议的网络通讯，它是无连接的，不可靠的，因为通讯双方发送数据后不知道对方是否已经收到数据，是否正常收到数据。任何一方建立一个socket以后就可以用sendto发送数据，也可以用recvfrom接收数据。根本不关心对方是否存在，是否发送了数据。它的特点是通讯速度比较快。大家都知道TCP是要经过三次握手的，而UDP没有。 

基于上述不同，UDP和TCP编程步骤也有些不同，如下：

TCP: 
TCP编程的服务器端一般步骤是： 
　　1、创建一个socket，用函数socket()； 
　　2、设置socket属性，用函数setsockopt(); * 可选 
　　3、绑定IP地址、端口等信息到socket上，用函数bind(); 
　　4、开启监听，用函数listen()； 
　　5、接收客户端上来的连接，用函数accept()； 
　　6、收发数据，用函数send()和recv()，或者read()和write(); 
　　7、关闭网络连接； 
　　8、关闭监听； 

TCP编程的客户端一般步骤是： 
　　1、创建一个socket，用函数socket()； 
　　2、设置socket属性，用函数setsockopt();* 可选 
　　3、绑定IP地址、端口等信息到socket上，用函数bind();* 可选 
　　4、设置要连接的对方的IP地址和端口等属性； 
　　5、连接服务器，用函数connect()； 
　　6、收发数据，用函数send()和recv()，或者read()和write(); 
　　7、关闭网络连接；

UDP:
与之对应的UDP编程步骤要简单许多，分别如下： 
　　UDP编程的服务器端一般步骤是： 
　　1、创建一个socket，用函数socket()； 
　　2、设置socket属性，用函数setsockopt();* 可选 
　　3、绑定IP地址、端口等信息到socket上，用函数bind(); 
　　4、循环接收数据，用函数recvfrom(); 
　　5、关闭网络连接； 

UDP编程的客户端一般步骤是： 
　　1、创建一个socket，用函数socket()； 
　　2、设置socket属性，用函数setsockopt();* 可选 
　　3、绑定IP地址、端口等信息到socket上，用函数bind();* 可选 
　　4、设置对方的IP地址和端口等属性; 
　　5、发送数据，用函数sendto(); 
　　6、关闭网络连接；

TCP和UDP是OSI模型中的运输层中的协议。TCP提供可靠的通信传输，而UDP则常被用于让广播和细节控制交给应用的通信传输。

UDP补充：
   UDP不提供复杂的控制机制，利用IP提供面向无连接的通信服务。并且它是将应用程序发来的数据在收到的那一刻，立刻按照原样发送到网络上的一种机制。即使是出现网络拥堵的情况下，UDP也无法进行流量控制等避免网络拥塞的行为。此外，传输途中如果出现了丢包，UDO也不负责重发。甚至当出现包的到达顺序乱掉时也没有纠正的功能。如果需要这些细节控制，那么不得不交给由采用UDO的应用程序去处理。换句话说，UDP将部分控制转移到应用程序去处理，自己却只提供作为传输层协议的最基本功能。UDP有点类似于用户说什么听什么的机制，但是需要用户充分考虑好上层协议类型并制作相应的应用程序。

TCP补充：
  TCP充分实现了数据传输时各种控制功能，可以进行丢包的重发控制，还可以对次序乱掉的分包进行顺序控制。而这些在UDP中都没有。此外，TCP作为一种面向有连接的协议，只有在确认通信对端存在时才会发送数据，从而可以控制通信流量的浪费。TCP通过检验和、序列号、确认应答、重发控制、连接管理以及窗口控制等机制实现可靠性传输。


TCP与UDP区别总结：
1、TCP面向连接（如打电话要先拨号建立连接）;UDP是无连接的，即发送数据之前不需要建立连接
2、TCP提供可靠的服务。也就是说，通过TCP连接传送的数据，无差错，不丢失，不重复，且按序到达;UDP尽最大努力交付，即不保   证可靠交付
3、TCP面向字节流，实际上是TCP把数据看成一连串无结构的字节流;UDP是面向报文的
  UDP没有拥塞控制，因此网络出现拥塞不会使源主机的发送速率降低（对实时应用很有用，如IP电话，实时视频会议等）
4、每一条TCP连接只能是点到点的;UDP支持一对一，一对多，多对一和多对多的交互通信
5、TCP首部开销20字节;UDP的首部开销小，只有8个字节
6、TCP的逻辑通信信道是全双工的可靠信道，UDP则是不可靠信道

```

###  五种I/O模型介绍一下 
```
https://segmentfault.com/a/1190000007355931
出处

阻塞 IO, 非阻塞 IO, 同步 IO, 异步 IO 这些术语相信有不少朋友都也不同程度的困惑吧? 我原来也是, 什么同步非阻塞 IO, 异步非阻塞 IO 的, 搞的头都大了. 后来仔细读了一遍
《UNIX 网络编程卷一 套接字联网 API(第三版)》的 6.2 章节, 终于把这些名词搞懂了.

下面我以《UNIX 网络编程卷一 套接字联网 API(第三版)》的 6.2 章节的内容为准, 整理了一下各种网络 IO 模型具体定义以及一些容易混淆的地方.
简介

Unix 下有 5 种可用的 IO 模型, 分别是:

    阻塞式 I/O

    非阻塞式 I/O

    I/O 复用(select 和 poll)

    信号驱动式 I/O (SIGIO)

    异步 I/O (POSIX 的 aio_系列函数)

阻塞式 I/O 模型

最流行的 IO 操作是阻塞式 IO(Blocking IO). 以 UDP 数据报套接字为例, 下图是其阻塞 IO 的调用过程:

clipboard.png

在上图中, 进程调用 recvfrom, 其系统调用直到数据报返回并且被复制到应用进程的缓冲区中 或者发送错误时才返回. 因此进程在调用 recvfrom 开始到它返回的整段时间内都是被阻塞的.
非阻塞式 IO(Non-Blocking IO)

进程把一个套接字设置为非阻塞是在通知内核: 当调用线程所请求的 IO 操作需要调用线程休眠来等待操作完成时, 此时不要将调用线程休眠, 而是返回一个错误.

clipboard.png

如上图所示, 前三次调用 recvfrom 时, 没有数据可返回, 因此内核转而立即返回一个 EWOULDBLOCK 错误. 第四次调用 recvfrom 时, 已经有数据了, 此时, recvfrom 会阻塞住, 等待内核将数据赋值到应用进程的缓冲区中, 然后再返回.(注意, 当有数据时, recvfrom 是阻塞的, 它会等待内核将数据复制到应用进程的缓冲区后, 才返回).
当一个应用进程像这样对一个非阻塞描述符循环调用 recvfrom 时, 我们称之为轮询(polling). 应用进程持续轮询内核, 以查看某个操作是否完成, 这么做会消耗大量的 CPU 时间, 不过这种模型偶尔也会遇到, 通常是专门提供某一种功能的系统中才有.
IO 复用模型

有了 IO 复用(IO multiplexing), 我们就可以调用 select 或 poll, 阻塞在这两个系统调用中的某一个之上, 而不是阻塞在真正的 IO 系统调用上. 例如:

clipboard.png

如上图所示, 当调用了 select 后, select 会阻塞住, 等待数据报套接字变为可读. 当 select 返回套接字可读这一条件时, 我们就可以调用 recvfrom 把所读取的数据报复制到应用进程缓冲区.
对比阻塞式 IO, IO 复用模型优势并不明显, 并且从使用方式来说, IO 复用模型还需要多调用一次 select, 因此从易用性上来说, 比阻塞式 IO 还略有不足. 不过 select 的杀手锏在于它可以监听多个文件描述符, 大大减小了阻塞线程的个数.
信号驱动 IO 模型

clipboard.png

信号驱动模型如上图所示. 当文件描述符就绪时, 我们可以让内核以信号的方式通知我们.
我们首先需要开启套接字的信号驱动式 IO 功能, 并通过 sigaction 系统调用安装一个信号处理函数. sigaction 系统调用是异步的, 它会立即返回. 当有数据时, 内核会给此进程发送一个 SIGIO 信号, 进而我们的信号处理函数就会被执行, 我们就可以在这个函数中调用 recvfrom 读取数据.
异步 IO 模型

异步 IO (asynchronous IO) 由 POSIX 规范定义, 在 POSIX 中定义了若干个异步 IO 的操作函数. 这个函数的工作原理是: 告知内核启动某个动作, 并让内核在整个操作(包括将数据从内核复制到应用进程缓冲区)完成后通知我们的应用进程.
异步 IO 模型和信号驱动的 IO 模型的主要区别在于: 信号驱动 IO 是由内核通知我们何时可以启动一个 IO 操作, 而异步 IO 模型是由内核通知我们 IO 操作何时完成.
异步 IO 模型的操作过程如图所示:

clipboard.png

当我们调用 aio_read 函数时(POSIX 异步 IO 函数以 aio_或 lio_ 开头), 给内核传递描述符, 缓冲区指针, 缓冲区大小(和 read 相同的三个参数) 和文件偏移(以 lseek 类似), 并告诉内核当整个操作完成时如何通知应用进程. 该系统调用立即返回, 而且在等待 IO 完成期间, 应用进程不被阻塞.
各种 IO 模型的比较

clipboard.png

如图所示, 上述五中 IO 模型中, 前四种模型(阻塞 IO, 非阻塞 IO, IO 复用, 信号驱动 IO)的主要区别在于第一阶段, 因为他们的第二阶段是一样的: 在数据从内核复制到调用者的缓冲区期间, 进程阻塞于 recvfrom 调用. 而第五种, 即异步 IO 模型中, 两个阶段都不需要应用进程处理, 内核为我们处理好了数据的等待和数据的复制过程.
关于同步 IO 和异步 IO

根据 POSIX 定义:

    A synchronous I/O operation causes the requesting process to be blocked until that I/O operation completes(导致请求进程阻塞, 直到 IO 操作完成).

    An asynchronous I/O operation does not cause the requesting process to be blocked(不导致请求进程阻塞).
    根据上述定义, 我们的前四种模型: 阻塞 IO 模型, 非阻塞 IO 模型, IO 复用模型和信号驱动 IO 模型都是同步 IO 模型, 因为其中真正的 IO 操作(recvfrom 调用) 会阻塞进程(因为当有数据时, recvfrom 会阻塞等待内核将数据从内核空间复制到应用进程空间, 当赋值完成后, recvfrom 才返回.) 只有异步 IO 模型与 POSIX 定义的异步 IO 相匹配.

总结

在处理网络 IO 操作时, 阻塞和非阻塞 IO 都是同步 IO.
只有调用了特殊的 API 才是异步 IO.

clipboard.png

因此网上常说的 "同步阻塞 IO", "同步非阻塞 IO" 其实就是阻塞 IO 模型和非阻塞 IO 模型, 因为阻塞 IO 和非阻塞 IO 模型都是同步的, 加了 "同步" 二字其实是多余了.
网络上常说的 "异步非阻塞 IO" 其实就是异步 IO 模型.

```

### 判断两个链表是否相交并找出交点
```
https://blog.csdn.net/jiqiren007/article/details/6572685

仔细研究两个链表，如果他们相交的话，那么他们最后的一个节点一定是相同的，否则是不相交的。因此判断两个链表是否相交就很简单了，分别遍历到两个链表的尾部，然后判断他们是否相同，如果相同，则相交；否则不相交。示意图如下：

判断出两个链表相交后就是判断他们的交点了。假设第一个链表长度为len1，第二个问len2，然后找出长度较长的，让长度较长的链表指针向后移动|len1 - len2| (len1-len2的绝对值)，然后在开始遍历两个链表，判断节点是否相同即可。


```

###  C++ map,set内部数据结构

```
C++ map,set内部数据结构

1 ）Set是一种关联容器，它用于存储数据，并且能从一个数据集合中取出数据。它的每个元素的值必须唯一，而且系统会根据该值来自动将数据排序。每个元素的值不能直接被改变。【重点】内部结构采用红黑树的平衡二叉树。multiset 跟set 类似，唯一的区别是允许键值重复！！！

如： 为何map和set的插入删除效率比用其他序列容器高？

为何每次insert之后，以前保存的iterator不会失效？

为何map和set不能像vector一样有个reserve函数来预分配数据？

当数据元素增多时（10000到20000个比较），map和set的插入和搜索速度变化如何？

或许有得人能回答出来大概原因，但要彻底明白，还需要了解STL的底层数据结构。 C++ STL 之所以得到广泛的赞誉，也被很多人使用，不只是提供了像vector, string, list等方便的容器，更重要的是STL封装了许多复杂的数据结构算法和大量常用数据结构操作。vector封装数组，list封装了链表，map和 set封装了二叉树等，在封装这些数据结构的时候，STL按照程序员的使用习惯，以成员函数方式提供的常用操作，如：插入、排序、删除、查找等。让用户在 STL使用过程中，并不会感到陌生。 C++ STL中标准关联容器set, multiset, map, multimap内部采用的就是一种非常高效的平衡检索二叉树：红黑树，也成为RB树(Red-Black Tree)。RB树的统计性能要好于一般的平衡二叉树(有些书籍根据作者姓名，Adelson-Velskii和Landis，将其称为AVL-树)，所以被STL选择作为了关联容器的内部结构。本文并不会介绍详细AVL树和RB树的实现以及他们的优劣，关于RB树的详细实现参看红黑树: 理论与实现(理论篇)。本文针对开始提出的几个问题的回答，来向大家简单介绍map和set的底层数据结构。

为何map和set的插入删除效率比用其他序列容器高？ 大部分人说，很简单，因为对于关联容器来说，不需要做内存拷贝和内存移动。说对了，确实如此。map和set容器内所有元素都是以节点的方式来存储，其节点结构和链表差不多，指向父节点和子节点。

  结构图可能如下：

       A

      /  /

    B    C

   / /   / /

  D  E F  G

因此插入的时候只需要稍做变换，把节点的指针指向新的节点就可以了。删除的时候类似，稍做变换后把指向删除节点的指针指向其他节点就OK了。这里的一切操作就是指针换来换去，和内存移动没有关系。 为何每次insert之后，以前保存的iterator不会失效？

看见了上面答案的解释，你应该已经可以很容易解释这个问题。iterator这里就相当于指向节点的指针，内存没有变，指向内存的指针怎么会失效呢(当然被删除的那个元素本身已经失效了)。相对于vector来说，每一次删除和插入，指针都有可能失效，调用push_back在尾部插入也是如此。因为为了保证内部数据的连续存放，iterator指向的那块内存在删除和插入过程中可能已经被其他内存覆盖或者内存已经被释放了。即使时push_back的时 候，容器内部空间可能不够，需要一块新的更大的内存，只有把以前的内存释放，申请新的更大的内存，复制已有的数据元素到新的内存，最后把需要插入的元素放 到最后，那么以前的内存指针自然就不可用了。特别时在和find等算法在一起使用的时候，牢记这个原则：不要使用过期的iterator。

为何map和set不能像vector一样有个reserve函数来预分配数据？ 我以前也这么问，究其原理来说时，引起它的原因在于在map和set内部存储的已经不是元素本身了，而是包含元素的节点。也就是说map内部使用的Alloc并不是map声明的时候从参数中传入的Alloc。例如： map, Alloc > intmap; 这时候在intmap中使用的allocator并不是Alloc, 而是通过了转换的Alloc，具体转换的方法时在内部通过Alloc::rebind重新定义了新的节点分配器，详细的实现参看彻底学习STL中的Allocator。其实你就记住一点，在map和set内面的分配器已经发生了变化，reserve方法你就不要奢望了。 当数据元素增多时（10000和20000个比较），map和set的插入和搜索速度变化如何？

如果你知道log2的关系你应该就彻底了解这个答案。在map和set中查找是使用二分查找，也就是说，如果有16个元素，最多需要比较4次就能找到结果，有32个元素，最多比较5次。那么有10000个呢？最多比较的次数为log10000，最多为14次，如果是20000个元素呢？最多不过15次。

看见了吧，当数据量增大一倍的时候，搜索次数只不过多了1次，多了1/14的搜索时间而已。你明白这个道理后，就可以安心往里面放入元素了。 最后，对于map和set Winter还要提的就是它们和一个c语言包装库的效率比较。在许多unix和linux平台下，都有一个库叫isc，里面就提供类似于以下声明的函数:

  void tree_init(void **tree);
  void *tree_srch(void **tree, int (*compare)(), void *data);
  void tree_add(void **tree, int (*compare)(), void *data, void (*del_uar)());
  int tree_delete(void **tree, int (*compare)(), void *data,void (*del_uar)());
  int tree_trav(void **tree, int (*trav_uar)());
  void tree_mung(void **tree, void (*del_uar)()); 

许多人认为直接使用这些函数会比STL map速度快，因为STL map中使用了许多模板什么的。其实不然，它们的区别并不在于算法，而在于内存碎片。如果直接使用这些函数，你需要自己去new一些节点，当节点特别多， 而且进行频繁的删除和插入的时候，内存碎片就会存在，而STL采用自己的Allocator分配内存，以内存池的方式来管理这些内存，会大大减少内存碎 片，从而会提升系统的整体性能。Winter在自己的系统中做过测试，把以前所有直接用isc函数的代码替换成map，程序速度基本一致。当时间运行很长 时间后（例如后台服务程序），map的优势就会体现出来。从另外一个方面讲，使用map会大大降低你的编码难度，同时增加程序的可读性。何乐而不为？

```

### STL 优缺点

```

作者：姚冬
链接：https://www.zhihu.com/question/20201972/answer/41324520
来源：知乎
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。

1. 代码膨胀问题每一个实例化过的模板类，都会膨胀出一份独立的代码，比如std::vector<std::string>, std::vector<int>，编译后会产生两份代码，在VC2008下，每份代码大约是3-4kb，这是因为vector比较简单代码少，如果是map则会产生30-50kb的代码，因为map里有个复杂的红黑树。对于数据处理类的代码里一般会定义很多种不同的结构体，不同的结构体放到不同的容器里，就会实例化出很多个类的代码，我见过一个项目里，这样的vector就有数百个。2. 内存使用效率问题 （以vc++2008为例）stl在内存使用效率上是比较低效的，比如std::string，它的sizeof大概是28，因为它有一个内置的16字节数组，用来做小字符串优化的，就是说低于16字节的字符串都会至少占用28字节内存，如果刚好17字节字符串，则会占用28字节+额外分配的字符串内存，额外分配的内存是一个堆块，又有很多浪费，相比用一个char *存储字符串大约多占用了一倍内存。还有map<>，每一个map的node都是一块独立分配的内存，如果是 map<int, int>呢，那就很悲剧了，为了存一个int要消耗几十个字节，很浪费的。如果元素数量有百万级，那么内存占用就很可观了，这种情况下建议自己实现allocator，做内存池。3. deep copy问题让两个容器的实例做赋值操作，看起来就一条语句，实际上容器里的每个元素都执行了一次赋值操作。如果容器里有百万级的数据，那么一个等号就产生了几百万次的构造和析构。传递参数的时候一定要用 const 引用，赋值可以用 swap代替。4. 隐式类型转换比如 有个函数void doSomething(const std::string &str);调用的时候   doSomething("hello");能编译执行，但是会产生一个临时的匿名的std::string实例，把"hello"复制一遍，然后在调用完成后析构掉。如果这个发生在循环体内部有可能影响性能。以上这些问题，在小程序里或者数据规模不大的时候，比如容器内元素只有几千这个规模，都不是什么大问题，那时开发效率才是重点，但是一旦有大数据stl容器会成为性能瓶颈的。我并不是主张不用STL，而是要充分了解STL的优缺点，根据应用场景做选择。


作者：知乎用户
链接：https://www.zhihu.com/question/20201972/answer/30182504
来源：知乎
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。

stl甚至C++的某些特性，都是在某些情况下进行取舍的。脱离这些具体的开发环境和需求，是无法讨论这个问题的。MMORPG服务端开发时，对CPU和内存的控制是很严格的。单线程支撑2000个玩家，16FPS，分配给每个玩家每帧的CPU只有80000个cycle（2.4GHz）。一个敌我关系判断函数，一秒钟可以调用上千万次。一个小对象可以同时存在上千万个。我们的策略是主要对象不用stl map，而使用自制的对象容器，这个容器集内存管理、ID分配、对象查找、遍历等功能于一体，因为可以根据需求有针对性的优化，所以性能提升很多。100k个对象的情况下，insert和delete的开销是stl的20%，find是50%。但其实很多管理的对象数量在几千以下的情况时，各种操作频率每秒最多几百次的，为了方便也是使用stl map的。stl list很多地方也用，但是stl list有很大的欠缺，就是无法很高效的删除一个节点。所以凡是有此需求的地方，我们都使用自制的list。vector一般不用，因为和直接写数组比没啥优势。很多时候，对性能瓶颈做优化的时候，是要连数据结构一起优化的，会有很多特定的需求，这种时候一般都不会使用stl。我们的代码风格偏C，大量使用平铺式的内存布局，这样数据可以很方便的memcpy后进行网络发送，或者读写文件，不需要写很多蛋疼的序列化。这样的结构里就不能使用stl了。algorithm里面用的也很少，因为需求不大。因为很多具体的策略是某些算法的组合，使用标准算法来拼，会很麻烦。另外，碎片化也是一个考量，但是没有10年前那么明显了。我们有一种将一个对象的大多数数据，都存放在一个完整的内存布局中的倾向，不喜欢有指针到处指。一个是这样cache命中的效率会高很多，更重要的是这样统计内存开销的时候，很容易知道哪个类的开销大，我们只要将对象管理器里的数量乘上sizeof的结果就行了。服务器端的内存都是自己的，比如一般逻辑服务器32G，数据服务器72G，我们都会倾向于一启动就用到50%的内存，反正不用白不用。

在STL中，list是一个双向循环链表，所谓循环链表就是指链表的头部和尾部是连接在一起的，下面两段代码实现的功能是一样的，但是执行过程却有所不同：

    //第一种
    list<int> lst1;
    if (lst1.empty())
    {
        //do something
    }
     
    //第二种
    list<int>lst2;
    if (lst2.size() == 0)
    {
        //do something
    }

       上面两段代码执行的功能都一样的，都是判断list集合内是否有元素，但是实际运行的过程却有所不同。
       对于第一种方法，由于list是首尾相连的，因此lst1直接判断头部的下一个节点是否为NULL就知道list是否有节点了，其时间复杂度为O(1)；
       对于第二种方法，size()函数的作用是获取节点的个数，因此lst2会从头到尾遍历一次链表，以得到链表节点的个数，其时间复杂度为O(n)。
       可见，虽然代码量差不多，但是时间复杂度上却相去甚远，当节点数较多的时候，两种写法相差的时间会越加明显。因此，当需要判断list的是否存在节点时，强烈建议使用list的empty()函数。
       
仅仅是个选择的问题，都是STL，可能写出来的效率相差几倍；
熟悉以下条款，高效的使用STL；
当对象很大时，建立指针的容器而不是对象的容器

1）STL基于拷贝的方式的来工作，任何需要放入STL中的元素，都会被复制；
这也好理解，STL工作的容器是在堆内开辟的一块新空间，而我们自己的变量一般存放在函数栈或另一块堆空间中；为了能够完全控制STL自己的元素，为了能在自己的地盘随心干活；这就涉及到复制；
而如果复制的对象很大，由复制带来的性能代价也不小 ；
对于大对象的操作，使用指针来代替对象能消除这方面的代价；
2）只涉及到指针拷贝操作， 没有额外类的构造函数和赋值构造函数的调用；

vecttor <BigObj> vt1;
vt1.push_bach(myBigObj);

vecttor <BigObj* > vt2;
vt2.push_bach(new BigObj());

注意事项：
1）容器销毁前需要自行销毁指针所指向的对象；否则就造成了内存泄漏；
2）使用排序等算法时，需要构造基于对象的比较函数，如果使用默认的比较函数，其结果是基于指针大小的比较，而不是对象的比较；
用empty() 代替size()来检查是否为空

因为对于list，size()会遍历每一个元素来确定大小，时间复杂度 o（n），线性时间；而empty总是保证常数时间；
尽量用区间成员函数代替单元素操作

使用区间成员函数有以下好处：
1）更少的函数调用
2）更少的元素移动
3）更少的内存分配

例：将v2后半部的元素赋值给v1：
单元式操作：

for (vector<Widget>::const_iterator ci = v2.begin() + v2.size() / 2;
ci != v2.end();
++ci)
v1.push_back(*ci)

使用区间成员函数assign()：

v1.assign(v2.begin() + v2.size() / 2, v2.end()); 

使用reserver避免不必要的内存分配(for vector)

新增元素空间不够时，vector会进行如下操作：
1）分配当前空间的两倍空间；
2）将当前元素拷贝到新的空间中；
3）释放之前的空间；
4）将新值放入新空间指定位置；

如果预先知道空间的大小，预先分配了空间避免了重新分配空间和复制的代价；
注：reserve()只是修改了容量，并非大小，向vector中增加元素还是需要通过push_back加入；
使用有序的vector代替关联容器(阶段性的操作适用)

对阶段性操作的定义：
先做一系列插入、完成之后，后续操作都是查询；

在阶段性的操作下，使用vector有以下优势：
1）因为vector有序，关联容器带来的有序优势散失；
2）都是使用二分法查找的前提下，查询算法对连续的内存空间的访问要快于离散的空间；
在map的insert()和operator［］中仔细选择

插入时，insert效率高；因为operator会先探查是否存在这个元素，如果不存在就构造一个临时的，然后才涉及到赋值，多了一个临时对象的构造；
更新时，［］效率更高，insert会创造一个对象，然后覆盖一个原有对象；而［］是在原有的对象上直接赋值操作；

散列函数的默认比较函数是equal＿to，因为不需要保持有序；
尽量用算法替代手写的循环

1）效率相比手写更高；
STL的代码都是C++专家写出来的，专家写出来的代码在效率上很难超越；
除非我们放弃了某些特性来满足特定的需求，可能能快过stl；比如，基于特定场合下的编程，放弃通用性，可移植性；
2）不容易出错；
3）使用高层次思维编程
相比汇编而言，C是高级语言；一条C语言语句，用汇编写需要好几条；
同样的，在STL的世界中，我们也有高层次的术语：
高层次的术语：insert／find／for＿each（STL算法）
低层次的词汇：for ／while（C++语法）
用高层次术语来思考编程，会更简单；
尽量用成员函数代替同名的算法

1）基于效率考虑，成员函数知道自己这个容器和其他容器有哪些特有属性，能够利用到这些特性；而通用算法不可以；
2）对于关联容器，成员函数find基于等价搜索；而通用算法find基于相等来搜索；可能导致结果不一样；
使用函数对象代替裸函数作为算法的输入参数

因为内联，在函数对象的方式中，内联有效，而作为函数指针时，一般编译器都不会内联函数指针指向的函数；即使指定了inline；
比如：

inline bool doubleGreater(double d1, double d2)
{
    return dl > d2;
}
vector<double> v;
...
sort(v.begin(), v.end(), doubleGreater);

这个调用不是真的把doubleGreater传给sort，它传了一个doubleGreater的指针。
更好的方式是使用函数对象：

sort(v.begin(), v.end(), greater<double>())

注：《effcient c＋＋》中的实验结论，使用函数对象一般是裸函数的1.5倍，最多能快2倍多
选择合适的排序算法

需要排序前思考我们的必要需求，可能我们只是需要前多少个元素，这时并不需要使用sort这种线性时间的工具，性能消耗更少的parttition可能是更好的选择；
以下算法的效率从左到右依次递减：

partition > stable_partition / nth_element / patical_sort / sort / stable_sort

功能说明：
partition ：将集合分隔为满足和不满足某个标准两个区间；
stable_partition ：partition的稳定版本；
nth_element ：获取任意顺序的前N个元素；
patical_sort ：获取前N个元素，这个N个元素已排序；
sort：排序整个区间；
stable_sort：sort的稳定版本；
选择合适的容器

为什么vector不提供push_front()成员方法？因为效率太差，如果有太多从前面插入的需求，就不应该使用vector，而用list；
关心查找速度，首先应该考虑散列容器（非标准STL容器,如：unordered_map,unordered_set)；其次是排序的vector，然后是标准的关联容器；       
       

```



